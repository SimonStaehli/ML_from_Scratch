{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbourRegressor(BaseEstimator):\n",
    "    \"\"\"\n",
    "    THis class implements K-Nearest Neighbour algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, neighbours=10, weights='uniform'):\n",
    "        \"\"\"\n",
    "        KNNRegression, which searches closest data points in the dataset and predicts y according to them.\n",
    "        \n",
    "        The distance, which is used to calculate the neighbours is euclidean: sqrt((x1-y1)**2 + (x2-y2)**2 ....)\n",
    "        \n",
    "        Params:\n",
    "        ----------------------------------\n",
    "        neighbours:          Number of neighbours in a neighbourhood. \n",
    "                             Lower values tend to overfit the training data and high amount to underfit                             \n",
    "\n",
    "        weights:             weight the averaging in the neighbourhood for the prediction\n",
    "                             \"uniform\": uniform weights simple mean\n",
    "                             \"distance\": 1/distance * value (weighted by inverse of the distance)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.neighbours = neighbours\n",
    "        self.weights = weights\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        (array, array) --> None\n",
    "        \n",
    "        Fits the model to a trainingset and defines the neighbourhoods.\n",
    "                \n",
    "        \"\"\"\n",
    "        y = y.reshape(-1,1)\n",
    "        \n",
    "        # Save Train Data to history as it will be used later for prediction\n",
    "        self.X_train = X \n",
    "        self.y_train = y\n",
    "        \n",
    "        return self\n",
    "                      \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        (array(nxm)) --> array (nx1)\n",
    "        \n",
    "        This method predicts X on a given Trainingset, which is also saved in history\n",
    "        Procedure:\n",
    "        for each row vector in X:\n",
    "            subtract it from all rowvec in X and take the norm or distance defined\n",
    "            sort by ascending distance\n",
    "            take the N closest to the rowvec\n",
    "            take mean of the N-closest to the rowvec\n",
    "            save it\n",
    "            \n",
    "        returns: Predictions for given X\n",
    "        \n",
    "        \"\"\"\n",
    "        predictions = np.array([])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            # Calculate norm distances between row vec and matrix\n",
    "            distances = self._calculate_distances(row_vec=X[i].T, X=self.X_train)\n",
    "            \n",
    "            # Concatenate together and sort\n",
    "            distances = np.concatenate((distances.reshape(-1,1), self.y_train), axis=1)\n",
    "            best_pred = distances[distances[:,0].argsort()][:self.neighbours]\n",
    "            \n",
    "            # Take averages\n",
    "            if self.weights == 'uniform':\n",
    "                predictions = np.append(predictions, np.mean(best_pred[:,1]))\n",
    "            elif self.weights == 'distance':\n",
    "                predictions = np.append(predictions, np.average(best_pred[:,1], weights=1/best_pred[:,0]))\n",
    "            else:\n",
    "                raise NotImplementedError('This weight is not available. Choose between \"uniform\" and \"distance\"')\n",
    "                    \n",
    "        return predictions\n",
    "        \n",
    "    @staticmethod\n",
    "    def _calculate_distances(row_vec, X):\n",
    "        \"\"\"\n",
    "        Calculates distance between row vectors.       \n",
    "        \"\"\"\n",
    "        # Subtraction via broadcasting the row vector of x gets subtracted at each row of X \n",
    "        # and the norm calculated along each column\n",
    "        distances = np.linalg.norm(X - row_vec, axis=1)\n",
    "        \n",
    "        return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn1 = KNeighborsRegressor(n_neighbors=5)\n",
    "knn1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.78, 22.9 , 25.36, 26.06, 27.1 , 27.1 , 20.88, 19.1 , 18.4 ,\n",
       "       19.48])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1 = knn1.predict(X)\n",
    "predictions1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNearestNeighbourRegressor(neighbours=5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn2 = KNearestNeighbourRegressor(neighbours=5)\n",
    "knn2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.78, 22.9 , 25.36, 26.06, 27.1 , 27.1 , 20.88, 19.1 , 18.4 ,\n",
       "       19.48])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2 = knn2.predict(X)\n",
    "predictions2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(predictions1, predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Murphy, K. P. (2012). Machine learning: a probabilistic perspective. MIT press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
